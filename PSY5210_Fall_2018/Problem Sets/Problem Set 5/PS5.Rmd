---
title: "PS5"
author: "Prateek Kumar"
date: "14 October 2018"
output: word_document
---
#Problem 1. Understanding the power of a test.
```{r}
set.seed(100)
##this function generates n data points extracts the p/value 
##bayes factor of the one-sample two-sided test for each:
simdata1 <- function(n,mean=.1)
{
  data <- rnorm(n,mean=mean)
  
  c( t.test(data)$p.value,
    binom.test(sum(data>0),n)$p.value,
    exp(ttestBF(data)@bayesFactor$bf)) 
  ##exponentiate because bf is stored as a log and so 0 is unbiased.
  
}

runs <- 500
##this simulates 1000 experiments:
null <- data.frame(pval=rep(NA,runs),
                           pvalnp=rep(NA,runs),
                           bayesf=rep(NA,runs))

##this simulates 1000 experiments:
simulation <- data.frame(pval=rep(NA,runs),
                           pvalnp=rep(NA,runs),
                           bayesf=rep(NA,runs))

n<-10   ##this is how many samples are drawn in each experiment
for(i in 1:runs)
{
  simulation[i,] <- simdata1(n)
  null[i,] <- simdata1(n,0)
}

# par(mfrow=c(2,3))
# hist(simulation$pval,col="navy",xlab="Standard t-test p value")
# hist(simulation$pvalnp,col="navy",xlab="Binomial test p value")
# hist(log(simulation$bayesf),col="navy", xlab="log(Bayes factor)")
# hist(null$pval,col="navy",xlab="Standard t-test p value")
# hist(null$pvalnp,col="navy",xlab="Binomial test p value")
# hist(log(null$bayesf),col="navy", xlab="log(Bayes factor)")

# Examine the number of significant tests when there is a true difference
mean(simulation$pval < .05) # significance for standard t-test
mean(simulation$pvalnp < .05) # number in binomial test
mean(simulation$bayesf > 2) # bayes factor greater than 2 
mean(simulation$bayesf < (-2))

# Examine number of significant tests under the null hypothesis:
mean(null$pval < .05) # number found significant in null distribution
mean(null$pvalnp < .05)# number in binomial test
mean(null$bayesf > 2 )  #evidence for the alternative ## bayes factor greater than 2 
mean(null$bayesf < (-2)) ##evidence for the null?

n<-50   ##this is how many samples are drawn in each experiment
for(i in 1:runs)
{
  simulation[i,] <- simdata1(n)
  null[i,] <- simdata1(n,0)
}

# Examine the number of significant tests when there is a true difference
mean(simulation$pval < .05) # significance for standard t-test
mean(simulation$pvalnp < .05) # number in binomial test
mean(simulation$bayesf > 2) # bayes factor greater than 2 
mean(simulation$bayesf < (-2))

# Examine number of significant tests under the null hypothesis:
mean(null$pval < .05) # number found significant in null distribution
mean(null$pvalnp < .05)# number in binomial test
mean(null$bayesf > 2 )  #evidence for the alternative ## bayes factor greater than 2 
mean(null$bayesf < (-2)) ##evidence for the null?

n<-200   ##this is how many samples are drawn in each experiment
for(i in 1:runs)
{
  simulation[i,] <- simdata1(n)
  null[i,] <- simdata1(n,0)
}

# Examine the number of significant tests when there is a true difference
mean(simulation$pval < .05) # significance for standard t-test
mean(simulation$pvalnp < .05) # number in binomial test
mean(simulation$bayesf > 2) # bayes factor greater than 2 
mean(simulation$bayesf < (-2))

# Examine number of significant tests under the null hypothesis:
mean(null$pval < .05) # number found significant in null distribution
mean(null$pvalnp < .05)# number in binomial test
mean(null$bayesf > 2 )  #evidence for the alternative ## bayes factor greater than 2 
mean(null$bayesf < (-2)) ##evidence for the null?

n<-1000   ##this is how many samples are drawn in each experiment
for(i in 1:runs)
{
  simulation[i,] <- simdata1(n)
  null[i,] <- simdata1(n,0)
}

# Examine the number of significant tests when there is a true difference
mean(simulation$pval < .05) # significance for standard t-test
mean(simulation$pvalnp < .05) # number in binomial test
mean(simulation$bayesf > 2) # bayes factor greater than 2 
mean(simulation$bayesf < (-2))

# Examine number of significant tests under the null hypothesis:
mean(null$pval < .05) # number found significant in null distribution
mean(null$pvalnp < .05)# number in binomial test
mean(null$bayesf > 2 )  #evidence for the alternative ## bayes factor greater than 2 
mean(null$bayesf < (-2)) ##evidence for the null?

n<-10000   ##this is how many samples are drawn in each experiment
for(i in 1:runs)
{
  simulation[i,] <- simdata1(n)
  null[i,] <- simdata1(n,0)
}

# Examine the number of significant tests when there is a true difference
mean(simulation$pval < .05) # significance for standard t-test
mean(simulation$pvalnp < .05) # number in binomial test
mean(simulation$bayesf > 2) # bayes factor greater than 2 
mean(simulation$bayesf < (-2))

# Examine number of significant tests under the null hypothesis:
mean(null$pval < .05) # number found significant in null distribution
mean(null$pvalnp < .05)# number in binomial test
mean(null$bayesf > 2 )  #evidence for the alternative ## bayes factor greater than 2 
mean(null$bayesf < (-2)) ##evidence for the null?

```

Problem 2: t-tests
```{r echo=FALSE}
data_q2 <- c(101,103,99,92,110,105,103,102,104,106,101,
            101,101,102,103,101,99,104,105,102,102,103,
            245,103,107,101,103,108,104,101,101,102)

# calculating the mean, sd, and se 
mu <- mean(data_q2)
sd <- sd(data_q2)
se <- sd/sqrt(length(data_q2))

# calculate t-test with mean - 100
t_test <- (mu-100)/se
print(paste0("t = ", round(t_test, 4)))

# calculate onetail p-value, from t-test and degree of freedom
p_value <- 1 - pt(t_test, length(data_q2)-1)
print(paste0("p-value = ", round(p_value, 4)))

# checking with t.test() function
t.test(data_q2, mu=100, alt="greater") 
```

Here we can see that the t and p-value we got through hands-on is same as that we got through the function t.test(). Also the average(mean) is reliably greater than 100.

```{r echo=FALSE}
# removing 245 from the vector
data_q2B <- c(101,103,99,92,110,105,103,102,104,106,101,
             101,101,102,103,101,99,104,105,102,102,103,
             103,107,101,103,108,104,101,101,102)

#estimate its mean, sd, and se
mu1 <- mean(data_q2B)
sd1 <- sd(data_q2B)
se1 <- sd1/sqrt(length(data_q2B))

#calculate t-test with mean - 100
t_test1 <- (mu1-100)/se1
print(paste0("t = ", t_test1))

#calculate onetail p-value, from t-test and degree of freedom
p_value1 <- 1 - pt(t_test1, length(data_q2B)-1)
print(paste0("p-value = ", p_value1))

t.test(data_q2B, mu=100, alt="greater") 
```
Here as well we can see that the t and p-value we got through hands-on is same as that we got through the function t.test(). Also the average(mean) is reliably greater than 100.

The difference in both the cases is that in the vector including 245 has p-value greater than 0.05 whereas when we exclude that from our vector we get the p-value approx to 0. So in the first case we fail to reject the null hypothesis whereas in second case since the p-value < 0.05 we reject the null hypothesis so we can conclude that 245 is the strongest evidence for this kind of behaviour.

Problem 3. Wilcox test
```{r echo=FALSE}
x <- rnorm(20)
y <- sort(x)

# Independent Wilcox Test
wilcox.test(x, y, paired = F, exact = F, alternative = "g") # equivalent to Mann-Whitney Test

# Paired Wilcox Test
wilcox.test(x, y, paired = T, exact = F, alternative = "g")
```
In the independent samples Wilcox Test we get the W value as 200 as we can see in the above output. The first Wilcox Test i.e. the independent samples Wilcox test is equivalent to Mann-Whitney Test where we get the p-value = 0.5054 and in the second Wilcox Test we get the p-value as 0.4435, we can see that in the paired Wilcox test the p-value is less this might be because of the correlation between the values of x and y because we are calculating it pairwise and so paired Wilcox Test cannot compute the exact p-value which we get in the independent samples Wilcox Test.

Problem 4. Comparing t-test, wilcox, and Bayes factor t-test
```{r echo=FALSE}
library(BayesFactor)

n <- 500
t_values <- c()
w_values <- c()
b_values <- c()

# running for 500 times
for(i in 1:n)
{  
  x <- rnorm(150)
  y <- rnorm(150) + .3
  
  t <- t.test(x, y)
  t_values<- append(t_values,t$p.value)
  #print(t_values)
  w <- wilcox.test(x, y, paired = F, exact = F, alternative = "g")
  w_values<- append(w_values,w$p.value)
  #print(w_values)
  b <- ttestBF(x, y)
  b_values<- append(b_values,b@bayesFactor$bf)
  #print(b_values)
}

print(t_values)
print(w_values)
print(b_values)

# Basic histogram
par(mfrow=c(1,3))
hist(t_values, main = 'p-values for T-Test', xlab = 'p_values', border = 'red', col = 'gold', las=1)
lines(density(t_values))
hist(w_values, main = 'p-values for Wilcox Test', xlab = 'p_values', border = 'red', col = 'gold', las=1)
lines(density(w_values))
hist(b_values, main = 'bf value for Bayes Factor T-Test', xlab = 'bf', border = 'red', col = 'gold', las=1)
lines(density(b_values))
```

We can see in the above outputs that the results differ for the different tests.

Here I ran the 3 tests for 500 times and we can see a pattern for the p-values for both the t-test and wilcox test. We see that the frequency of p-value = 0.1 is more than 400 out of 500 test results which means that the p-value is close to 0.1 approx 95-99% times. And the count decreases substantially as the value approaches 1.

For the Bayes factor T-test I plotted the histogram for the bf value and we can see that the frequency of the bf value is is also highest at 1 and next at -1 and decreases substantially as it approaches towards the positive values. 

Problem 5. Robustness to transforms
```{r echo=FALSE}
library(BayesFactor)

n <- 500
t_values <- c()
w_values <- c()
b_values <- c()

# running for 500 times
for(i in 1:n)
{  
  x <- exp(rnorm(150))
  y <- exp(rnorm(150) + .3)
  
  t <- t.test(x, y)
  t_values<- append(t_values,t$p.value)
  w <- wilcox.test(x, y, paired = F, exact = F, alternative = "g")
  w_values<- append(w_values,w$p.value)
  b <- ttestBF(x, y)
  b_values<- append(b_values,b@bayesFactor$bf)
}

# Basic histogram
par(mfrow=c(1,3))
hist(t_values, main = 'p-values for T-Test', xlab = 'p_values', border = 'red', col = 'gold', las=1)
lines(density(t_values))
hist(w_values, main = 'p-values for Wilcox Test', xlab = 'p_values', border = 'red', col = 'gold', las=1)
lines(density(w_values))
hist(b_values, main = 'bf value for Bayes Factor T-Test', xlab = 'bf', border = 'red', col = 'gold', las=1)
lines(density(b_values))
```
Here as well I ran the 3 tests for 500 times and we can see a similar pattern for the p-values for both the t-test and wilcox test as seen in Q4. We see that the frequency of p-value = 0.1 is still highest for both but now the count has decreased to 350-380 whereas earlier it was 400+. For Wilcox Test the decrease in frequency for p-value = 0.1 is less as compared to that in t-test. And we can also see that the count decreases substantially as the values reach 1.

For the Bayes factor T-test I plotted the histogram for the bf value and we can see that the frequency of the bf value is is now highest at -2 and next at -1 and decreases substantially as it approaches towards the positive values. 

In Q4 we are using the rnorm() to create the vector so the vector will have mean = 0 so the values are evenly distributed across the value 0 for x and y(approx) and when we apply the exp() on the values we get the positive results for them and the mean is between 1-3 so because of all the elements in the vactor in exp() is positive we see such change in the values for the 3 tests.

Problem 6. Comparison to paired tests.
```{r echo=FALSE}
n <- 500
t_values <- c()
w_values <- c()
b_values <- c()

# running for 500 times
for(i in 1:n)
{  
  x <- rnorm(150)
  y <- rnorm(150) + .3
  
  t <- t.test(x,y,paired=T)
  t_values<- append(t_values,t$p.value)
  w <- wilcox.test(x, y, paired = T, exact = FALSE, alternative = "g")
  w_values<- append(w_values,w$p.value)
  b <- ttestBF(x,y,paired=T)
  b_values<- append(b_values,b@bayesFactor$bf)
}

# Basic histogram
par(mfrow=c(1,3))
hist(t_values, main = 'p-values for T-Test', xlab = 'p_values', border = 'red', col = 'gold', las=1)
lines(density(t_values))
hist(w_values, main = 'p-values for Wilcox Test', xlab = 'p_values', border = 'red', col = 'gold', las=1)
lines(density(w_values))
hist(b_values, main = 'bf value for Bayes Factor T-Test', xlab = 'bf', border = 'red', col = 'gold', las=1)
lines(density(b_values))
```
Here I ran the 3 paired tests for 500 times.
For T-test we cannot see any change in the pattern for the paired and the independent test, but for wilcox test we can see that the histogram is now its mirror image. Earlier the frequency was highest for 0.1 but now its for p-value = 1.
And for Bayes factor T-test also we cannot see much of the change between the paired and the independent tests.

Problem 7: Correlations
```{r echo=FALSE}
x <- runif(100)
y <- runif(100)

z <- x + .5*y
z1 <- z
z2 <- z+10
z3 <- log(z)
z4 <- z*10
z5 <- z + runif(100)

#the pearson and spearman correlations
#x and z1
p1 <- cor.test(x,z1,method="pearson")
s1 <- cor.test(x,z1,method="spearman")

#x and z2
p2 <- cor.test(x,z2,method="pearson")
s2 <- cor.test(x,z2,method="spearman")

#x and z3
p3 <- cor.test(x,z3,method="pearson")
s3 <- cor.test(x,z3,method="spearman")

#x and z4
p4 <- cor.test(x,z4,method="pearson")
s4 <- cor.test(x,z4,method="spearman")

#x and z5
p5 <- cor.test(x,z5,method="pearson")
s5 <- cor.test(x,z5,method="spearman")

p <- cbind(p1=p1$estimate, p2=p2$estimate, p3=p3$estimate, p4=p4$estimate, p5=p5$estimate)
s <- cbind(s1=s1$estimate, s2=s2$estimate, s3=s3$estimate, s4=s4$estimate, s5=s5$estimate)

p
s

#Bayes factor
bf1<-ttestBF(x, z1)
bf1@bayesFactor$bf
bf2<-ttestBF(x, z2)
bf2@bayesFactor$bf
bf3<-ttestBF(x, z3)
bf3@bayesFactor$bf
bf4<-ttestBF(x, z4)
bf4@bayesFactor$bf
bf5<-ttestBF(x, z5)
bf5@bayesFactor$bf
#print(bf1@)
```
Here we can see that among the pearson correlation output the correlation between x and z3 and between x and z5 is less as compared to that of z1,z2 and z4. And in spearman correlation we see that only the correlation between x and z5 is less whereas correlation between x and other z values is same.

For pearson correlation we can see that all the values are greater than 0.5 so we have strong correlation with all the z values but correlation between x and z1,z2 & z4 with form a better staright line than that with z3 & z5.

For spearman correlation we can see that the strength between x and other z values except z5 is more.

Looking at the bayes factor for each correlation we see that its highest for x and z2 and is lowest for x and z1.